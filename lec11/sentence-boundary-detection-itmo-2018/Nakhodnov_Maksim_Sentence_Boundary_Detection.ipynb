{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Process, Queue\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "import json\n",
    "import io\n",
    "import re\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "with io.open('train_data.json','r',encoding='utf8') as f:\n",
    "    for line in f.readlines():\n",
    "        d = json.loads(line)\n",
    "        train_data.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = []\n",
    "with io.open('test_data.json','r',encoding='utf8') as f:\n",
    "    for line in f.readlines():\n",
    "        d = json.loads(line)\n",
    "        test_data.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reform_data(Data, index_bias, res_queue):\n",
    "    Index = index_bias\n",
    "    result = []\n",
    "    for data in Data:\n",
    "        marks = []\n",
    "        good_marks = set()\n",
    "        par = data['Paragraph']\n",
    "        pos = -1\n",
    "        for i in re.findall(re.compile(u'[!\"…\\.»?]', re.U), par):\n",
    "            pos = par.find(i, pos + 1, len(par))\n",
    "            marks += [{u'Index': Index, u'Pos': pos, u'Mark': par[pos], u'Label': False}]\n",
    "            Index += 1\n",
    "            \n",
    "        for sentence in data[u'Sentences']:\n",
    "            good_marks.add(par.find(sentence) + len(sentence) - 1)\n",
    "        \n",
    "        for mark in marks:\n",
    "            if(mark['Pos'] in good_marks):\n",
    "                mark['Label'] = True\n",
    "        result += [{'Paragraph': par, 'Marks': marks}]\n",
    "    res_queue.put(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "chuncs_non_ref = np.array_split(train_data, 24)\n",
    "\n",
    "train_reform = []\n",
    "processes = []\n",
    "res_queue = Queue() \n",
    "WORKER_NUM = len(chuncs_non_ref)\n",
    "for i in xrange(WORKER_NUM):\n",
    "    process = Process(target=reform_data, args=(chuncs_non_ref[i], 100000 * i, res_queue))\n",
    "    processes.append(process)\n",
    "    process.start()\n",
    "    \n",
    "complete_workers = 0\n",
    "while complete_workers != WORKER_NUM:\n",
    "    item = res_queue.get()\n",
    "    complete_workers += 1\n",
    "    train_reform += item\n",
    "    print complete_workers\n",
    "        \n",
    "for process in processes: process.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLabelOneHotEncoder():\n",
    "    def __init__(self):\n",
    "        self.label_enc = LabelEncoder()\n",
    "        self.one_hot_enc = OneHotEncoder()\n",
    "        self.result_len = 0\n",
    "        \n",
    "    def fit(self, array):\n",
    "        self.label_enc.fit(array)\n",
    "        self.one_hot_enc.fit(self.label_enc.transform(array).reshape(-1, 1))\n",
    "        self.result_len = len(array)\n",
    "        \n",
    "    def predict_one(self, label):\n",
    "        return self.one_hot_enc.transform(self.label_enc.transform([label])[0]).toarray()[0]\n",
    "    \n",
    "    def predict_none(self):\n",
    "        return np.zeros(self.result_len, dtype=np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_of_sentences = np.array([u'!', u'\"', u'.', u'?', u'\\xbb', u'\\u2026'])\n",
    "end_encoder = MyLabelOneHotEncoder()\n",
    "end_encoder.fit(end_of_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_ends = [u'!', u'…', u'.', u'?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_cand = Counter()\n",
    "for p in train_reform:\n",
    "    par = p['Paragraph']\n",
    "    for cand in p['Marks']:\n",
    "        pos = cand['Pos']\n",
    "        ## _'alpha'._\n",
    "        if (pos >= 3 and pos + 1 < len(par) and\n",
    "            par[pos - 2] == ' ' and par[pos] == '.' and\n",
    "            par[pos - 1].isalpha() and par[pos - 1].islower() and\n",
    "            par[pos + 1] == ' '):\n",
    "            stop_words_cand[par[pos - 1:pos + 1]] += 1\n",
    "        ## _'alpha''alpha'._\n",
    "        if (pos >= 3 and pos + 1 < len(par) and\n",
    "            par[pos - 3] == ' ' and\n",
    "            par[pos - 2].isalpha() and par[pos - 2].islower() and\n",
    "            par[pos - 1].isalpha() and par[pos - 1].islower() and\n",
    "            par[pos] == '.' and par[pos + 1] == ' '):\n",
    "            stop_words_cand[par[pos - 2:pos + 1]] += 1\n",
    "        ## _'alpha'.'alpha'._\n",
    "        if (pos >= 4 and pos + 1 < len(par) and\n",
    "            par[pos - 4] == ' ' and\n",
    "            par[pos - 3].isalpha() and par[pos - 3].islower() and\n",
    "            par[pos - 2] == '.' and\n",
    "            par[pos - 1].isalpha() and par[pos - 1].islower() and\n",
    "            par[pos] == '.' and par[pos + 1] == ' '):\n",
    "            stop_words_cand[par[pos - 3:pos + 1]] += 1\n",
    "        ## _'alpha'._'alpha'._    \n",
    "        if (pos >= 5 and pos + 1 < len(par) and\n",
    "            par[pos - 5] == ' ' and\n",
    "            par[pos - 4].isalpha() and par[pos - 4].islower() and\n",
    "            par[pos - 3] == '.' and par[pos - 2] == ' ' and\n",
    "            par[pos - 1].isalpha() and par[pos - 1].islower() and\n",
    "            par[pos] == '.' and par[pos + 1] == ' '):\n",
    "            stop_words_cand[par[pos - 4:pos + 1]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "г. 163\n",
      "т. 108\n",
      "т.е. 88\n",
      "т.д. 57\n",
      "ст. 57\n",
      "им. 54\n",
      "т.п. 54\n",
      "л.д. 52\n",
      "см. 40\n",
      "н. 39\n",
      "гг. 31\n",
      "д. 27\n",
      "же. 25\n",
      "ч. 24\n",
      "км. 24\n",
      "м. 22\n",
      "он. 21\n",
      "в. 19\n",
      "е. 19\n",
      "мм. 19\n",
      "др. 18\n",
      "их. 16\n",
      "т. д. 16\n",
      "т.к. 16\n",
      "с. 16\n",
      "т. е. 15\n",
      "э. 14\n",
      "н. э. 14\n",
      "кв. 14\n",
      "ее. 14\n",
      "я. 13\n",
      "бы. 13\n",
      "пр. 10\n",
      "п. 10\n",
      "ул. 9\n",
      "её. 9\n",
      "кг. 9\n",
      "св. 8\n",
      "т. п. 7\n",
      "да. 7\n",
      "m. 7\n",
      "т. н. 7\n",
      "гр. 6\n"
     ]
    }
   ],
   "source": [
    "for word, cnt in stop_words_cand.most_common():\n",
    "    if cnt > 5:\n",
    "        print word, cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_one = set([u' г. ', u' т. ', u' н. ', u' д. ',\n",
    "                      u' ч. ', u' м. ', u' е. ', u' в. ',\n",
    "                      u' с. ', u' э. ', u' п. '])\n",
    "stop_words_two = set([u' им. ', u' ст. ', u' др. ', u' ул. ',\n",
    "                      u' вв. ', u' см. ', u' гг. ', u' кв. ',\n",
    "                      u' св. ', u' км. ', u' мм. '])\n",
    "stop_words_thr = set([u' т.е. ', u' т.д. ', u' т.п. ', u' л.д. ',\n",
    "                      u' т.к ', u' т.н. ', u' p.m. '])\n",
    "stop_words_fou = set([u' т. д. ', u' т. е. ', u' н. э. ', u' т. п. ',\n",
    "                      u' т. н '])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_encoder_one = MyLabelOneHotEncoder()\n",
    "stop_encoder_one.fit(list(stop_words_one))\n",
    "\n",
    "stop_encoder_two = MyLabelOneHotEncoder()\n",
    "stop_encoder_two.fit(list(stop_words_two))\n",
    "\n",
    "stop_encoder_thr = MyLabelOneHotEncoder()\n",
    "stop_encoder_thr.fit(list(stop_words_thr))\n",
    "\n",
    "stop_encoder_fou = MyLabelOneHotEncoder()\n",
    "stop_encoder_fou.fit(list(stop_words_fou))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "11\n",
      "7\n",
      "5\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "print stop_encoder_one.result_len\n",
    "print stop_encoder_two.result_len\n",
    "print stop_encoder_thr.result_len\n",
    "print stop_encoder_fou.result_len\n",
    "print end_encoder.result_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_code(string, code_len=None):\n",
    "    if not code_len:\n",
    "        code_len = len(string)\n",
    "    if len(string) > code_len:\n",
    "        raise RuntimeError('Code len must be greater string len')\n",
    "    tmp_str = np.array(list(ord(c) for c in string))\n",
    "    return np.pad(tmp_str, [(0, code_len - tmp_str.shape[0])], mode='constant', constant_values=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_encode_substrs(Data, window):\n",
    "    X_tmp = []\n",
    "    for data in Data:\n",
    "        par = data['Paragraph']\n",
    "        for cand in data['Marks']:\n",
    "            pos = cand['Pos']            \n",
    "            substr = par[pos - window[0]:pos + window[1]]\n",
    "            X_tmp.append(string_to_code(substr, window[0] + window[1]))\n",
    "    return X_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encode_substrs = extract_encode_substrs(train_reform, [5, 2])\n",
    "test_encode_substrs = extract_encode_substrs(test_data, [5, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(categorical_features='all', dtype=<type 'numpy.float64'>,\n",
       "       handle_unknown='error', n_values='auto', sparse=True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "substrs_encoder = OneHotEncoder()\n",
    "substrs_encoder.fit(train_encode_substrs + test_encode_substrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1184)\n"
     ]
    }
   ],
   "source": [
    "print substrs_encoder.transform(np.zeros(7).reshape(1,-1)).toarray().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_data = np.zeros((26476,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "fea_num = 51 + 1184"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(par, cand):\n",
    "    features = np.zeros([1, 0])\n",
    "    is_eof = False                      # 0\n",
    "    is_next_space = False               # 1\n",
    "    is_next_in_ends = False             # 2\n",
    "    is_next_alpha_upper = False         # 3\n",
    "    is_initial = False                  # 4\n",
    "    is_numerated_list_beg = False       # 5\n",
    "    is_in_good_ends = False             # 6\n",
    "    is_city_or_year = False             # 7\n",
    "    # Сложные правила:\n",
    "    ## ._'digit'._'upper'\n",
    "    is_next_list_begin = False          # 8\n",
    "    # +Нестрогие\n",
    "    ## 'upper'._'upper'\n",
    "    is_feature_9 = False                # 9\n",
    "    ## 'upper''lower'._'upper'\n",
    "    is_feature_10 = False               # 10\n",
    "    is_is_stop_words = False            # 11\n",
    "    \n",
    "    pos = cand['Pos']\n",
    "    \n",
    "    # 0\n",
    "    if pos == len(par) - 1:\n",
    "        is_eof = True\n",
    "    features = np.append(features, is_eof)\n",
    "    # 1\n",
    "    if pos + 1 < len(par) and par[pos + 1] == ' ':\n",
    "        is_next_space = True\n",
    "    features = np.append(features, is_next_space)\n",
    "    # 2\n",
    "    if pos + 2 < len(par) and par[pos + 1] == ' ' and par[pos + 2].isalpha() and par[pos + 2].isupper():\n",
    "        is_next_alpha_upper = True\n",
    "    features = np.append(features, is_next_alpha_upper)\n",
    "    # 3\n",
    "    if pos + 1 < len(par) and par[pos + 1] in end_of_sentences:\n",
    "        is_next_in_ends = True\n",
    "    features = np.append(features, is_next_in_ends)\n",
    "    # 4\n",
    "    # 'upper'._'upper'.\n",
    "    # 'upper'.'upper'.\n",
    "    if ((pos >= 4 and \n",
    "         par[pos - 4].isalpha() and par[pos - 4].isupper() and \n",
    "         par[pos - 3] == '.' and par[pos - 2] == ' ' and \n",
    "         par[pos - 1].isalpha() and par[pos - 1].isupper() and \n",
    "         par[pos] == '.') or \n",
    "        (pos >= 3 and \n",
    "         par[pos - 3].isalpha() and par[pos - 3].isupper() and \n",
    "         par[pos - 2] == '.' and \n",
    "         par[pos - 1].isalpha() and par[pos - 1].isupper() and \n",
    "         par[pos] == '.')):\n",
    "        is_initial = True\n",
    "    features = np.append(features, is_initial)\n",
    "    # 5\n",
    "    if ((pos >= 1 and par[pos] == '.' and par[pos - 1].isdigit()) or \n",
    "        (pos >= 2 and par[pos] == '.' and par[pos - 1].isdigit() and par[pos - 2].isdigit())):\n",
    "        is_numerated_list_beg = True\n",
    "    features = np.append(features, is_numerated_list_beg)\n",
    "    # 6\n",
    "    if cand['Mark'] in good_ends:\n",
    "        is_in_good_ends = True\n",
    "    features = np.append(features, is_in_good_ends)\n",
    "    # 7\n",
    "    if pos >= 2 and pos + 1 < len(par) and par[pos - 2:pos + 2] == u' г. ':\n",
    "        is_city_or_year = True\n",
    "    features = np.append(features, is_city_or_year)\n",
    "    # 8\n",
    "    ## ._d._A\n",
    "    ## ._dd._A\n",
    "    ## ._d.A\n",
    "    ## ._dd.A\n",
    "    if (pos + 5 < len(par) and \n",
    "        par[pos] == '.' and par[pos + 1] == ' ' and\n",
    "        par[pos + 2].isdigit() and\n",
    "        par[pos + 3] == '.' and par[pos + 4] == ' ' and\n",
    "        par[pos + 5].isalpha() and par[pos + 5].isupper()):\n",
    "        is_next_list_begin = True\n",
    "    if (pos + 6 < len(par) and \n",
    "        par[pos] == '.' and par[pos + 1] == ' ' and\n",
    "        par[pos + 2].isdigit() and\n",
    "        par[pos + 3].isdigit() and\n",
    "        par[pos + 4] == '.' and par[pos + 5] == ' ' and\n",
    "        par[pos + 6].isalpha() and par[pos + 6].isupper()):\n",
    "        is_next_list_begin = True\n",
    "    if (pos + 4 < len(par) and \n",
    "        par[pos] == '.' and par[pos + 1] == ' ' and\n",
    "        par[pos + 2].isdigit() and\n",
    "        par[pos + 3] == '.' and\n",
    "        par[pos + 4].isalpha() and par[pos + 4].isupper()):\n",
    "        is_next_list_begin = True\n",
    "    if (pos + 5 < len(par) and \n",
    "        par[pos] == '.' and par[pos + 1] == ' ' and\n",
    "        par[pos + 2].isdigit() and\n",
    "        par[pos + 3].isdigit() and\n",
    "        par[pos + 4] == '.' and\n",
    "        par[pos + 5].isalpha() and par[pos + 5].isupper()):\n",
    "        is_next_list_begin = True\n",
    "    features = np.append(features, is_next_list_begin)\n",
    "    # 9\n",
    "    if (pos >= 1 and pos + 2 < len(par) and\n",
    "        par[pos - 1].isalpha() and par[pos - 1].isupper() and\n",
    "        par[pos] == '.' and par[pos + 1] == ' ' and\n",
    "        par[pos + 2].isalpha() and par[pos + 2].isupper()):\n",
    "        is_feature_9 = True\n",
    "    features = np.append(features, is_feature_9)\n",
    "    # 10\n",
    "    if (pos >= 2 and pos + 2 < len(par) and\n",
    "        par[pos - 2].isalpha() and par[pos - 2].isupper() and\n",
    "        par[pos - 1].isalpha() and par[pos - 1].islower() and\n",
    "        par[pos] == '.' and par[pos + 1] == ' ' and\n",
    "        par[pos + 2].isalpha() and par[pos + 2].isupper()):\n",
    "        is_feature_10 = True  \n",
    "    features = np.append(features, is_feature_10)\n",
    "    # 11\n",
    "    ## _'alpha'._\n",
    "    if (pos >= 2 and pos + 1 < len(par) and par[pos - 2:pos + 2] in stop_words_one):\n",
    "        features = np.append(features, stop_encoder_one.predict_one(par[pos - 2:pos + 2]))\n",
    "    else:\n",
    "        features = np.append(features, stop_encoder_one.predict_none())\n",
    "    # 12\n",
    "    ## _'alpha''alpha'._\n",
    "    if (pos >= 3 and pos + 1 < len(par) and par[pos - 3:pos + 2] in stop_words_two):\n",
    "        features = np.append(features, stop_encoder_two.predict_one(par[pos - 3:pos + 2]))\n",
    "    else:\n",
    "        features = np.append(features, stop_encoder_two.predict_none())\n",
    "    # 13\n",
    "    ## _'alpha'.'alpha'._\n",
    "    if (pos >= 4 and pos + 1 < len(par) and par[pos - 4:pos + 2] in stop_words_thr):\n",
    "        features = np.append(features, stop_encoder_thr.predict_one(par[pos - 4:pos + 2]))\n",
    "    else:\n",
    "        features = np.append(features, stop_encoder_thr.predict_none())\n",
    "    # 14\n",
    "    ## _'alpha'._'alpha'._\n",
    "    if (pos >= 5 and pos + 1 < len(par) and par[pos - 5:pos + 2] in stop_words_fou):\n",
    "        features = np.append(features, stop_encoder_fou.predict_one(par[pos - 5:pos + 2]))\n",
    "    else:\n",
    "        features = np.append(features, stop_encoder_fou.predict_none())\n",
    "\n",
    "    # append type of mark\n",
    "    features = np.append(features, end_encoder.predict_one(par[pos]))\n",
    "            \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(Data, res_queue):\n",
    "    Substr_tmp = []\n",
    "    X_tmp = np.zeros([0, fea_num])\n",
    "    y_tmp = np.zeros([0, 1])\n",
    "    \n",
    "    chunk_encode_substrs = extract_encode_substrs(Data, [5,2])\n",
    "    chunk_one_hot_codes = substrs_encoder.transform(chunk_encode_substrs).toarray()\n",
    "    \n",
    "    idx = 0\n",
    "    for p in Data:\n",
    "        par = p['Paragraph']\n",
    "        for cand in p['Marks']:\n",
    "            features = predict(par, cand)\n",
    "            features = np.append(features, chunk_one_hot_codes[idx])\n",
    "            X_tmp = np.append(X_tmp, features.reshape(1, -1), axis=0)\n",
    "            y_tmp = np.append(y_tmp, cand['Label'])\n",
    "            Substr_tmp.append(par[max(0, cand['Pos'] - 15):min(len(par), cand['Pos'] + 15)])\n",
    "            \n",
    "            idx += 1\n",
    "    res_queue.put((X_tmp, y_tmp, Substr_tmp))\n",
    "    res_queue.put(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "chuncs = np.array_split(train_reform, 24)\n",
    "\n",
    "X = np.zeros([0, fea_num])\n",
    "y = np.zeros([0, 1])\n",
    "Substrs = []\n",
    "\n",
    "processes = []\n",
    "res_queue = Queue() \n",
    "WORKER_NUM = len(chuncs)\n",
    "for i in xrange(WORKER_NUM):\n",
    "    process = Process(target=extract_features, args=(chuncs[i], res_queue))\n",
    "    processes.append(process)\n",
    "    process.start()\n",
    "    \n",
    "complete_workers = 0\n",
    "while complete_workers != WORKER_NUM:\n",
    "    item = res_queue.get()\n",
    "    if item == None:\n",
    "        complete_workers += 1\n",
    "        print complete_workers\n",
    "    else:\n",
    "        X = np.append(X, item[0], axis=0)\n",
    "        y = np.append(y, item[1])\n",
    "        Substrs += item[2]\n",
    "        \n",
    "for process in processes: process.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21977.0 (30298,)\n",
      "44800.0 (61513,)\n"
     ]
    }
   ],
   "source": [
    "print np.sum(y_test), y_test.shape\n",
    "print np.sum(y_train), y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBClassifier(n_estimators=1000, max_depth=13, n_jobs=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=13, min_child_weight=1, missing=None, n_estimators=1000,\n",
       "       n_jobs=24, nthread=None, objective='binary:logistic',\n",
       "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "       seed=None, silent=True, subsample=1)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9872020520311922"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_predict, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "334\n"
     ]
    }
   ],
   "source": [
    "print np.sum(y_predict != y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "пондента» «S7-Ъ».\n",
      "1.0 0.0\n",
      "2. Судно, исполь\n",
      "0.0 1.0\n",
      " в дверь камеры.\n",
      "0.0 1.0\n",
      "главному выходу.\n",
      "1.0 0.0\n",
      " к ней в прачки. Но, глядя на \n",
      "0.0 1.0\n",
      "ми-гимназистами. Через неделю \n",
      "1.0 0.0\n",
      "ень, всю неделю. В конце же не\n",
      "0.0 1.0\n",
      "уга и знакомясь. Был один отст\n",
      "1.0 0.0\n",
      "т ссылку на них.\n",
      "1.0 0.0\n",
      "ней же комнате?». Прикручинилс\n",
      "1.0 0.0\n",
      "Отрыгис (галакт. Leto Atreides\n",
      "0.0 1.0\n",
      "му пышный приём. В этот момент\n",
      "1.0 0.0\n",
      "— Э-э-э… Спасибо за за\n",
      "1.0 0.0\n",
      " и Максимилиана.\n",
      "0.0 1.0\n",
      "ишь, просыпайся! Уже без пятна\n",
      "0.0 1.0\n",
      "щё одна пятёрка. Приятно иметь\n",
      "1.0 0.0\n",
      "или ему кулаком.\n",
      "0.0 1.0\n",
      "ией? Кто препод? — начал Митро\n",
      "1.0 0.0\n",
      "ло» больше всех.\n",
      "1.0 0.0\n",
      "равке и смеётся…\n",
      "0.0 1.0\n",
      "ользовался этим!\n",
      "0.0 1.0\n",
      " «любовные игры», но перерыв у\n",
      "0.0 1.0\n",
      "— 5 группа! Хм… Двадцать. А д\n",
      "1.0 0.0\n",
      "уда Вы денетесь. Но только не \n",
      "0.0 1.0\n",
      "рмально вообще?! Ну ничего, я \n",
      "0.0 1.0\n",
      " всё же удалась.\n",
      "1.0 0.0\n",
      "обще долбанутый?\n",
      "0.0 1.0\n",
      "настоящая днюха. Так что, без \n",
      "1.0 0.0\n",
      "уск в пять утра. Утром столь р\n",
      "1.0 0.0\n",
      "шёл в аудиторию. Отработка про\n",
      "1.0 0.0\n",
      "вливается опека.\n",
      "0.0 1.0\n",
      "о Суда РФ от 04.05.1990 N 4.\n",
      "1.0 0.0\n",
      "го расстройства.\n",
      "1.0 0.0\n",
      "уда РФ от 06.02.2004 N 52-О.\n",
      "1.0 0.0\n",
      " попечительства.\n",
      "0.0 1.0\n",
      "1. Обязанности п\n",
      "1.0 0.0\n",
      " под патронажем.\n",
      "1.0 0.0\n",
      "следующего года.\n",
      "1.0 0.0\n",
      "2. Орган опеки и\n",
      "1.0 0.0\n",
      "1. Оборот земель\n",
      "1.0 0.0\n",
      "йской Федерации.\n",
      "1.0 0.0\n",
      "б уплате налога.\n",
      "1.0 0.0\n",
      " уплате налогов.\n",
      "0.0 1.0\n",
      "льными законами.\n",
      "1.0 0.0\n",
      "сполнении сметы.\n",
      "0.0 1.0\n",
      "иципальных нужд. Выкуп части ж\n",
      "1.0 0.0\n",
      "не собственника.\n",
      "0.0 1.0\n",
      " известия о нем.\n",
      "1.0 0.0\n",
      "в районном суде. В этом случае\n",
      "1.0 0.0\n",
      "ы и другие дела.\n",
      "0.0 1.0\n",
      "2. Несовершеннол\n",
      "0.0 1.0\n",
      "тоящего Кодекса.\n",
      "1.0 0.0\n",
      "1. Объяснения ст\n",
      "0.0 1.0\n",
      "планы, чертежи).\n",
      "1.0 0.0\n",
      "твлялись записи.\n",
      "1.0 0.0\n",
      "дать заключение.\n",
      "0.0 1.0\n",
      "свое заключение.\n",
      "1.0 0.0\n",
      "ие вторых денег. Интуитивно да\n",
      "0.0 1.0\n",
      ", который нужен. В отсутствие \n",
      "1.0 0.0\n",
      "лась в культуру. Это, в частно\n",
      "0.0 1.0\n",
      "ь его в головах. Поэтому успех\n",
      "1.0 0.0\n",
      "ития демократии. Подобную идил\n",
      "1.0 0.0\n",
      " микроорганизмы. Макрофаги так\n",
      "1.0 0.0\n",
      " сайте конкурса.\n",
      "0.0 1.0\n",
      "менения климата.\n",
      "0.0 1.0\n",
      "днюю Коста-Рику.\n",
      "0.0 1.0\n",
      "о 10 тысяч евро. Но, подсчитав\n",
      "0.0 1.0\n",
      "ших предложений. Самые достойн\n",
      "1.0 0.0\n",
      "газета «Гардиан» (Guardian) со\n",
      "1.0 0.0\n",
      "калацию кризиса. Кроме того, о\n",
      "0.0 1.0\n",
      "(Duilio Contin). Дуилио Контин\n",
      "1.0 0.0\n",
      "т таких страниц. Противники но\n",
      "1.0 0.0\n",
      "вание «Гармония» модуль получи\n",
      "0.0 1.0\n",
      "далось потушить. Убыток оценив\n",
      "1.0 0.0\n",
      "естного солдата. Однако полици\n",
      "1.0 0.0\n",
      "слова «планета». С появлением \n",
      "1.0 0.0\n",
      "ма Плутон-Харон», т.е. система\n",
      "0.0 1.0\n",
      "ермин «планета». На вопрос зад\n",
      "1.0 0.0\n",
      "рится в докладе. Следует выявл\n",
      "1.0 0.0\n",
      "ами-сухогрузами. В тёмное врем\n",
      "0.0 1.0\n",
      " вот берег реки... Даже русско\n",
      "1.0 0.0\n",
      "ны или силикона! Аисты вынужде\n",
      "1.0 0.0\n",
      "нарёв, рядовые …ского кавалери\n",
      "0.0 1.0\n",
      "ковника с женою. Казалось, из \n",
      "0.0 1.0\n",
      "ожит его память. Мысли невольн\n",
      "0.0 1.0\n",
      "то мой ребёнок». Кроме того, к\n",
      "1.0 0.0\n",
      "ещал отчитаться.\n",
      "0.0 1.0\n",
      "на — нелегально. Мексика высту\n",
      "1.0 0.0\n",
      "ером во вторник. Ожидается, чт\n",
      "1.0 0.0\n",
      "селевых потоков. Никаких работ\n",
      "1.0 0.0\n",
      "заявила Клинтон.\n",
      "0.0 1.0\n",
      "вный недостаток. Мы должны уст\n",
      "1.0 0.0\n",
      "2. Государство с\n",
      "1.0 0.0\n",
      "нкурсной основе. Спортсмены, в\n",
      "1.0 0.0\n",
      "3. Учебно-тренир\n",
      "0.0 1.0\n",
      "блики Казахстан.\n",
      "1.0 0.0\n",
      " четырёх команд. Дважды резуль\n",
      "1.0 0.0\n",
      "нимаете, вырвал…\n",
      "1.0 0.0\n",
      "мо удивительный!\n",
      "0.0 1.0\n",
      "тен… переводчиц.\n",
      "0.0 1.0\n",
      "переводят. Беда!\n",
      "0.0 1.0\n",
      "Бумага свистит.\n",
      "0.0 1.0\n",
      "Не всё ли едино?.. Земная жизн\n",
      "1.0 0.0\n",
      "чи — ещё щедрей! Это соседство\n",
      "1.0 0.0\n",
      "олодным блеском.\n",
      "0.0 1.0\n",
      " отделаешься! А? Сто раз, и ни\n",
      "1.0 0.0\n",
      "ие снов кошмара. И эти воплоще\n",
      "0.0 1.0\n",
      "ь из возгласов \"эхма\" да \"ух\",\n",
      "0.0 1.0\n",
      "Толя, подумайте\". -- \"Подумаю,\n",
      "0.0 1.0\n",
      "о ж тебе погода? Ты пойми, чуд\n",
      "1.0 0.0\n",
      "ие — бери… бери…\n",
      "0.0 1.0\n",
      "о одинокую душу. С неприятным \n",
      "1.0 0.0\n",
      "есть, небольшой… Ну, да ведь —\n",
      "0.0 1.0\n",
      "ль! Великолепно! Восхитительно\n",
      "0.0 1.0\n",
      " мне! Не могу я! — Чудак, прав\n",
      "0.0 1.0\n",
      "одить в уныние?.. — с недоумен\n",
      "1.0 0.0\n",
      "намерением уйти.\n",
      "1.0 0.0\n",
      "изменным другом. Когда этот не\n",
      "0.0 1.0\n",
      "ическим хохотом. Глаза его дик\n",
      "1.0 0.0\n",
      "аешь… Эй, Семён! — кричит след\n",
      "1.0 0.0\n",
      "е спал всю ночь! Где ты была?\n",
      "1.0 0.0\n",
      " слове «омнибус», потому что о\n",
      "1.0 0.0\n",
      "ал в ресторанах. Как-то у них \n",
      "1.0 0.0\n",
      " них переколочу! Да меня, може\n",
      "0.0 1.0\n",
      "ее восприимчивы. Они все еще о\n",
      "0.0 1.0\n",
      "половое чувство. Чувство это г\n",
      "1.0 0.0\n",
      "ляется еще ярче. Два банкира, \n",
      "1.0 0.0\n",
      "цифику развития.\n",
      "1.0 0.0\n",
      " этого процесса.\n",
      "1.0 0.0\n",
      "торых (1945 г.р.). И один — по\n",
      "0.0 1.0\n",
      " дело вот в чём. Если описание\n",
      "1.0 0.0\n",
      "ечному регрессу.\n",
      "1.0 0.0\n",
      "овия и привычки. Цель данного \n",
      "0.0 1.0\n",
      "вас получится!). И жизненна он\n",
      "0.0 1.0\n",
      "ли православном?) «покаянии» н\n",
      "1.0 0.0\n",
      "» в «Сатириконе», спектакль аб\n",
      "0.0 1.0\n",
      "начинает работу.\n",
      "1.0 0.0\n",
      "аписей в блогах. Технология бу\n",
      "1.0 0.0\n",
      "Английский клуб»? Декадентские\n",
      "1.0 0.0\n",
      "ался со всеми... и начал расск\n",
      "1.0 0.0\n",
      "и с апельсинами. Вы не видите \n",
      "0.0 1.0\n",
      " интернет-черви?\n",
      "1.0 0.0\n",
      "не расслабишься.\n",
      "0.0 1.0\n",
      "гентно сказали \"Здравствуйте\".\n",
      "1.0 0.0\n",
      "висит от страны. Претензии род\n",
      "1.0 0.0\n",
      "иков моря» Гюго.\n",
      "1.0 0.0\n",
      "абирается домой. По технологии\n",
      "0.0 1.0\n",
      "мира в Германии.\n",
      "1.0 0.0\n",
      "— 767,6 тыс. кв. км, 17,7 тыс.\n",
      "1.0 0.0\n",
      "зиатской России. Жителям европ\n",
      "1.0 0.0\n",
      "НЕЗАПНО и нету!\" - ибо при нал\n",
      "1.0 0.0\n",
      "ссийского эфира. Причём, уверя\n",
      "0.0 1.0\n",
      "ак бы не забыть! Пошли, голубч\n",
      "1.0 0.0\n",
      "— думает Зайкин.\n",
      "1.0 0.0\n",
      "тишины и покоя?»\n",
      "1.0 0.0\n",
      "олжать вытекать.\n",
      "0.0 1.0\n",
      "на создание ЕВФ. Хотя главный \n",
      "0.0 1.0\n",
      "о победили. вот.\n",
      "0.0 1.0\n",
      ". Чуть не забыл! GNOME не пред\n",
      "1.0 0.0\n",
      "льной мощности \"на крайний слу\n",
      "1.0 0.0\n",
      "сь левый лагерь.\n",
      "0.0 1.0\n",
      "ичество голосов.\n",
      "1.0 0.0\n",
      "расны. «Спартак» предстал боев\n",
      "0.0 1.0\n",
      "олчал, а «Зенит» ощутимо подсе\n",
      "1.0 0.0\n",
      "ой изображают...\n",
      "1.0 0.0\n",
      "ичьего аппарата. Пока Борис Не\n",
      "1.0 0.0\n",
      " делать с вором? Посадить в тю\n",
      "1.0 0.0\n",
      "должит воровать.\n",
      "0.0 1.0\n",
      "ого «бизнесмена» я бы обложила\n",
      "1.0 0.0\n",
      "дного имущества. Кому в резуль\n",
      "1.0 0.0\n",
      "оловному делу №..., Пупкин В.В\n",
      "0.0 1.0\n",
      "акой получилось. Причем курьер\n",
      "1.0 0.0\n",
      "связанных задач.\n",
      "0.0 1.0\n",
      "1. Каждое расчле\n",
      "0.0 1.0\n",
      "еременного тока», так как неяс\n",
      "0.0 1.0\n",
      "м (1969г., США).\n",
      "1.0 0.0\n",
      " проблемы, ...).\n",
      "0.0 1.0\n",
      "равления работы.\n",
      "1.0 0.0\n",
      "х условий и т.д. не усложняло \n",
      "1.0 0.0\n",
      " в производство. Порядок таких\n",
      "1.0 0.0\n",
      "следовательские. Изучают и уто\n",
      "1.0 0.0\n",
      "менный учет 2...5 критериев.\n",
      "1.0 0.0\n",
      "т критерии, т.е. упорядочено р\n",
      "1.0 0.0\n",
      "их ранжирования. В таблице по \n",
      "0.0 1.0\n",
      "с прочности и т.д., т.е. все т\n",
      "0.0 1.0\n",
      "номичность, т.е. точность полу\n",
      "1.0 0.0\n",
      " условий работы.\n",
      "1.0 0.0\n",
      "ассматриваемого.\n",
      "1.0 0.0\n",
      "ения, фиктивный».\n",
      "1.0 0.0\n",
      "ция превосходна» — это утвержд\n",
      "0.0 1.0\n",
      "их нагрузок и т.п. В настоящее\n",
      "0.0 1.0\n",
      " нагрузок и т.п. В настоящее в\n",
      "1.0 0.0\n",
      "их операций и т.п.), чтобы обе\n",
      "1.0 0.0\n",
      "и всех) заказов.\n",
      "1.0 0.0\n",
      "знаку (рис. 12). Анализ показы\n",
      "1.0 0.0\n",
      "3.4. Исследование \n",
      "0.0 1.0\n",
      " уровне способа. Патентная чис\n",
      "0.0 1.0\n",
      "кта воздействия. Например, из \n",
      "1.0 0.0\n",
      " в конце работы. Часто к проек\n",
      "1.0 0.0\n",
      "тивного решения.\n",
      "1.0 0.0\n",
      "ось по мнению В.П. Дьяченко ра\n",
      "1.0 0.0\n",
      "Р по версии А.М. Бирмана. Толь\n",
      "0.0 1.0\n",
      " органов власти.\n",
      "1.0 0.0\n",
      "термин «финансы» стал употребл\n",
      "1.0 0.0\n",
      "вать гравитацию.\n",
      "0.0 1.0\n",
      "e International».. 2007 году в\n",
      "0.0 1.0\n",
      " Алтайском крае.\n",
      "1.0 0.0\n",
      "дия не отвечает. Или откроешь \n",
      "0.0 1.0\n",
      "а излишка воды).\n",
      "1.0 0.0\n",
      "колько картинок.\n",
      "1.0 0.0\n",
      "раздо красивее». Ничего не пол\n",
      "1.0 0.0\n",
      "ва раза дороже). Посоветуйте, \n",
      "1.0 0.0\n",
      "будь тоже потом.\n",
      "0.0 1.0\n",
      "?.. Не хочется?.. Просто не вы\n",
      "0.0 1.0\n",
      "рограммы \"Кадры\", проводимую п\n",
      "1.0 0.0\n",
      "состав не зовем? Грузия может \n",
      "1.0 0.0\n",
      "аются по полной! Уважаю! Очень\n",
      "1.0 0.0\n",
      "ее благополучно.\n",
      "0.0 1.0\n",
      " Федерации) В.И. Ульянов (Лени\n",
      "1.0 0.0\n",
      "убивает фраза: \"Во избежание п\n",
      "0.0 1.0\n",
      "вляются важными? Я так думаю ч\n",
      "1.0 0.0\n",
      "кземпляр страха. Теперь у меня\n",
      "0.0 1.0\n",
      "огии свои идолы. Хотя в слове \n",
      "1.0 0.0\n",
      "ал (Михалыч, да?) тут всего-на\n",
      "0.0 1.0\n",
      " штаб-квартирах.\n",
      "1.0 0.0\n",
      "f Summer School. Причем я дума\n",
      "0.0 1.0\n",
      " задание к нему.\n",
      "0.0 1.0\n",
      "ться. Изучение \"специфических \n",
      "1.0 0.0\n",
      "ин к браузеру... А больше всег\n",
      "1.0 0.0\n",
      "что-то теряется. Если мне пона\n",
      "1.0 0.0\n",
      "аздо удобнее, т.е. есть сущест\n",
      "1.0 0.0\n",
      "утевых заметках\" Задорнова. Пр\n",
      "1.0 0.0\n",
      "ез особых ляпов.\n",
      "0.0 1.0\n",
      "нтина Заслонова. И вот где-то \n",
      "1.0 0.0\n",
      "без приключений. Маленькая тон\n",
      "0.0 1.0\n",
      "«Золотой глобус» за лучшую жен\n",
      "0.0 1.0\n",
      "кции Ихуд Леуми.\n",
      "1.0 0.0\n",
      " взгляд фуражку...\n",
      "1.0 0.0\n",
      "освал \"Аларих\"... Ну, там, бул\n",
      "0.0 1.0\n",
      " в этой команде. Удачи вам.\n",
      "0.0 1.0\n",
      "ерритории Земли. Космополитизм\n",
      "1.0 0.0\n",
      "3. Какими расшир\n",
      "1.0 0.0\n",
      " вместе с Лизой. Лиза недоволь\n",
      "1.0 0.0\n",
      "ла и до столицы.\n",
      "1.0 0.0\n",
      "ок из пробирки». В течение сле\n",
      "1.0 0.0\n",
      "r, Schott Solar. Проект получи\n",
      "0.0 1.0\n",
      "ян погибших нет.\n",
      "1.0 0.0\n",
      "пожимал плечами. Мы спустились\n",
      "0.0 1.0\n",
      "ободить бедняка. Он сидел непо\n",
      "0.0 1.0\n",
      "рюк приподнялся.\n",
      "1.0 0.0\n",
      "рвом, MTV и ДТВ. А несколько д\n",
      "1.0 0.0\n",
      "тересный оборот.\n",
      "1.0 0.0\n",
      "й Божией Матери. Ещё в середин\n",
      "0.0 1.0\n",
      "Крыльев Советов» и «Ростова» в\n",
      "1.0 0.0\n",
      "тивно сниматься. В 1961 году е\n",
      "0.0 1.0\n",
      "от лишнего веса.\n",
      "0.0 1.0\n",
      ", ФГУП ГКНПЦ им. М.В. Хруничев\n",
      "0.0 1.0\n",
      "одуль «Колумбус» и смонтирует \n",
      "0.0 1.0\n",
      "ове в 1895 году. В настоящее в\n",
      "1.0 0.0\n",
      "лен в парламент.\n",
      "0.0 1.0\n",
      "открытый космос.\n",
      "1.0 0.0\n",
      "х сахара крови). Таким образом\n",
      "0.0 1.0\n",
      "олочной кислоты. Основной прич\n",
      "0.0 1.0\n",
      " и калорийности. Следует полно\n",
      "1.0 0.0\n",
      "орного действия.\n",
      "1.0 0.0\n",
      "го шунтирования. Эти операции \n",
      "0.0 1.0\n",
      "видные поступки. Чтобы вновь в\n",
      "0.0 1.0\n",
      "бедняки Лондона», «главным фак\n",
      "0.0 1.0\n",
      "ответственность» писали, что п\n",
      "1.0 0.0\n",
      "ского свойства…». При этом, хо\n",
      "1.0 0.0\n",
      "логия / Под ред. Дж. Ф. Шели. \n",
      "1.0 0.0\n",
      ". Кудрявцева, В. Э. Эминова. 3\n",
      "0.0 1.0\n",
      "плюс один голос.\n",
      "0.0 1.0\n",
      "ой ночи, малыши», «В мире живо\n",
      "0.0 1.0\n",
      " Ш.: «Чемпионом» по нарушениям\n",
      "0.0 1.0\n",
      "о не изменилось! И вот сейчас,\n",
      "0.0 1.0\n",
      "ные изображения. Представитель\n",
      "1.0 0.0\n",
      "навести порядок. И снес девять\n",
      "1.0 0.0\n",
      "тябре 2010 года.\n",
      "1.0 0.0\n",
      "4. Лес или море?\n",
      "1.0 0.0\n",
      " Рафаэля Корреа.\n",
      "1.0 0.0\n",
      "ыли уволены 143. При этом боль\n",
      "1.0 0.0\n",
      "thon'a или Ruby. А может вообщ\n",
      "1.0 0.0\n",
      " = m²k² + 2mEL². Тогда существ\n",
      "1.0 0.0\n",
      "ипа, пишет «АиФ».\n",
      "0.0 1.0\n",
      "ие облик города. Инспекция ста\n",
      "0.0 1.0\n",
      " 54 млн т в год. Кроме того, I\n",
      "1.0 0.0\n",
      "аёт бодрый спич. «Творчески об\n",
      "1.0 0.0\n",
      "енщин? Не может!\n",
      "0.0 1.0\n",
      "в. Нет движения. Сплошные фрус\n",
      "0.0 1.0\n",
      "ечена в мраморе. Президентский\n",
      "0.0 1.0\n",
      "рокому читателю. Это общемиров\n",
      "1.0 0.0\n",
      "ане и «Отчаяние» по Набокову, \n",
      "0.0 1.0\n",
      "угой Фассбиндер.\n",
      "1.0 0.0\n",
      "о театра «Театр.doc», Театра и\n",
      "0.0 1.0\n",
      " Майклсон - зав. кафедрой соци\n",
      "0.0 1.0\n",
      "омических задач. Именно таким \n",
      "0.0 1.0\n",
      " через интернет.\n",
      "1.0 0.0\n",
      "илу с 1 февраля. Вето наложено\n",
      "0.0 1.0\n",
      "мными «свалками» спама, в кото\n",
      "1.0 0.0\n",
      "тературы online» от Сергея Бел\n",
      "1.0 0.0\n",
      "ва «Ад Маргинем», пожалуй, тол\n",
      "1.0 0.0\n",
      "ильм «Даунхаус».\n",
      "1.0 0.0\n",
      "находкой сезона\". Он финиширов\n",
      "1.0 0.0\n",
      "нашего общества. И в этой связ\n",
      "1.0 0.0\n",
      "ерики, 1934 год.\n",
      "1.0 0.0\n",
      "анитарные связи.\n",
      "1.0 0.0\n",
      "й первой книгой. Лишь увлечени\n",
      "0.0 1.0\n",
      "х не выдающаяся. Во всяком слу\n",
      "1.0 0.0\n",
      "янина в Москве».\n",
      "0.0 1.0\n",
      "шутками. Кошмар!\n",
      "0.0 1.0\n",
      "и яркие диалоги… для этого нуж\n",
      "0.0 1.0\n",
      "как хотелось бы. Причём обраща\n",
      "0.0 1.0\n",
      " операции исчез. Она сделала е\n",
      "0.0 1.0\n",
      "ние двух недель. Оказывается, \n",
      "0.0 1.0\n",
      "ерь — о главном. Как было точн\n",
      "0.0 1.0\n",
      " явно недостает. Разве так про\n",
      "1.0 0.0\n",
      " необязательным. И что же дела\n",
      "1.0 0.0\n",
      "том или Ходжалы? Но самое глав\n",
      "1.0 0.0\n",
      "аизма, буддизма. Ученики и их \n",
      "0.0 1.0\n",
      "воих сограждан».\n",
      "1.0 0.0\n",
      "Не то что живые.\n",
      "1.0 0.0\n",
      "ьезных проблем». При этом сена\n",
      "1.0 0.0\n",
      "кущем 2011 году. Большинству п\n",
      "1.0 0.0\n",
      "crosoft Windows.\n",
      "0.0 1.0\n",
      "унный механизм). Предназначала\n",
      "1.0 0.0\n",
      "различных машин.\n",
      "1.0 0.0\n",
      "восходительство.\n",
      "0.0 1.0\n",
      "опытами по пыли. Лица у них бы\n",
      "1.0 0.0\n",
      "ком царя Давида. Община почита\n",
      "0.0 1.0\n",
      "ловкость лисицы. В первом росс\n",
      "1.0 0.0\n",
      "ится яйцеклетка.\n",
      "0.0 1.0\n",
      "вер в заложники. К счастью, де\n",
      "1.0 0.0\n",
      "проблемах Эйрин. Лиам прогоняе\n",
      "1.0 0.0\n"
     ]
    }
   ],
   "source": [
    "for idx in xrange(len(y_predict)):\n",
    "    if y_predict[idx] != y_test[idx]:\n",
    "        print Substrs[idx]\n",
    "        print y_predict[idx], y_test[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_real = np.zeros([26476, fea_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n",
      "8000\n",
      "8100\n",
      "8200\n",
      "8300\n",
      "8400\n",
      "8500\n",
      "8600\n",
      "8700\n",
      "8800\n",
      "8900\n",
      "9000\n",
      "9100\n",
      "9200\n",
      "9300\n",
      "9400\n",
      "9500\n",
      "9600\n",
      "9700\n",
      "9800\n",
      "9900\n",
      "10000\n",
      "10100\n",
      "10200\n",
      "10300\n",
      "10400\n",
      "10500\n",
      "10600\n",
      "10700\n",
      "10800\n",
      "10900\n",
      "11000\n",
      "11100\n",
      "11200\n",
      "11300\n",
      "11400\n",
      "11500\n",
      "11600\n",
      "11700\n",
      "11800\n",
      "11900\n",
      "12000\n",
      "12100\n",
      "12200\n",
      "12300\n",
      "12400\n",
      "12500\n",
      "12600\n",
      "12700\n",
      "12800\n",
      "12900\n",
      "13000\n",
      "13100\n",
      "13200\n",
      "13300\n",
      "13400\n",
      "13500\n",
      "13600\n",
      "13700\n",
      "13800\n",
      "13900\n",
      "14000\n",
      "14100\n",
      "14200\n",
      "14300\n",
      "14400\n",
      "14500\n",
      "14600\n",
      "14700\n",
      "14800\n",
      "14900\n",
      "15000\n",
      "15100\n",
      "15200\n",
      "15300\n",
      "15400\n",
      "15500\n",
      "15600\n",
      "15700\n",
      "15800\n",
      "15900\n",
      "16000\n",
      "16100\n",
      "16200\n",
      "16300\n",
      "16400\n",
      "16500\n",
      "16600\n",
      "16700\n",
      "16800\n",
      "16900\n",
      "17000\n",
      "17100\n",
      "17200\n",
      "17300\n",
      "17400\n",
      "17500\n",
      "17600\n",
      "17700\n",
      "17800\n",
      "17900\n",
      "18000\n",
      "18100\n",
      "18200\n",
      "18300\n",
      "18400\n",
      "18500\n",
      "18600\n",
      "18700\n",
      "18800\n",
      "18900\n",
      "19000\n",
      "19100\n",
      "19200\n",
      "19300\n",
      "19400\n",
      "19500\n",
      "19600\n",
      "19700\n",
      "19800\n",
      "19900\n",
      "20000\n",
      "20100\n",
      "20200\n",
      "20300\n",
      "20400\n",
      "20500\n",
      "20600\n",
      "20700\n",
      "20800\n",
      "20900\n",
      "21000\n",
      "21100\n",
      "21200\n",
      "21300\n",
      "21400\n",
      "21500\n",
      "21600\n",
      "21700\n",
      "21800\n",
      "21900\n",
      "22000\n",
      "22100\n",
      "22200\n",
      "22300\n",
      "22400\n",
      "22500\n",
      "22600\n",
      "22700\n",
      "22800\n",
      "22900\n",
      "23000\n",
      "23100\n",
      "23200\n",
      "23300\n",
      "23400\n",
      "23500\n",
      "23600\n",
      "23700\n",
      "23800\n",
      "23900\n",
      "24000\n",
      "24100\n",
      "24200\n",
      "24300\n",
      "24400\n",
      "24500\n",
      "24600\n",
      "24700\n",
      "24800\n",
      "24900\n",
      "25000\n",
      "25100\n",
      "25200\n",
      "25300\n",
      "25400\n",
      "25500\n",
      "25600\n",
      "25700\n",
      "25800\n",
      "25900\n",
      "26000\n",
      "26100\n",
      "26200\n",
      "26300\n",
      "26400\n"
     ]
    }
   ],
   "source": [
    "real_encode_substrs = extract_encode_substrs(test_data, [5,2])\n",
    "real_one_hot_codes = substrs_encoder.transform(real_encode_substrs).toarray()\n",
    "\n",
    "\n",
    "idx = 0\n",
    "for p in test_data:\n",
    "    par = p['Paragraph']\n",
    "    for cand in p['Marks']:\n",
    "        if idx % 100 == 0:\n",
    "            print idx\n",
    "        features = predict(par, cand)\n",
    "        features = np.append(features, real_one_hot_codes[idx])\n",
    "        X_real[idx] = features.reshape(1, -1)\n",
    "        \n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_real = model.predict(X_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18019.0 (26476,)\n"
     ]
    }
   ],
   "source": [
    "print np.sum(y_real), y_real.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "fd = open('marks.txt', 'w')\n",
    "for p in test_data:\n",
    "    par = p['Paragraph']\n",
    "    fd.write(par.encode('utf-8') + '\\n')\n",
    "    for cand in p['Marks']:\n",
    "        out_data[cand['Index']-1] = y_real[idx]\n",
    "        pos = cand['Pos']\n",
    "        fd.write(par[max(0,pos-15):pos].encode('utf-8') + '!' + par[pos:min(len(par),pos+15)].encode('utf-8') + ' ' + str(out_data[cand['Index']-1][0]) + '\\n')\n",
    "        \n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(out_data, columns=['Mark'], index=range(1,26477))\n",
    "df.index.name = 'Id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"sampleSubmission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
